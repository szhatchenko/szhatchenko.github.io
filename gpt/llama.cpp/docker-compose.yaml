version: "3"
services:
  llama-cpp:
    image: developmentontheedge/llama.cpp
    container_name: llama-cpp
    network_mode: host
    restart: unless-stopped
    volumes:
      - ./chat.sh:/chat/chat.sh:ro
      - ./chat.html:/chat/chat.html:ro
      - ./models:/chat/models:ro
    command: ./websocketd --staticdir=. --port=9093 ./chat.sh ggml-model-7b-q4_0.bin
  vicuna:
    image: developmentontheedge/llama.cpp
    container_name: vicuna
    network_mode: host
    restart: unless-stopped
    volumes:
      - ./chat.sh:/chat/chat.sh:ro
      - ./chat.html:/chat/chat.html:ro
      - ./models:/chat/models:ro
    command: ./websocketd --staticdir=. --port=9094 ./chat.sh ggml-vicuna-7b-4bit-rev1.bin
